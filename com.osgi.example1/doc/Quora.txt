https://www.quora.com/Why-does-Windows-still-use-NTFS-Why-not-ext4-the-file-system-for-Linux-since-it-actively-prevents-disk-fragmentation
Why does Windows still use NTFS? Why not ext4, the file system for Linux, since it actively prevents disk fragmentation?

Tony Mason
Tony Mason, 30+ years developing systems software (UNIX, Windows, etc.)
Answered Mar 18, 2018 · Upvoted by Besada Hanna, Senior Software engineer , Micorosoft Evangelist and Daniel Melanchthon, Senior Partner Development Manager at Microsoft (2005-present)
Interesting answers thus far. I’ll share my own uncommon perspective on this: I worked on a journaling file system (for UNIX systems) that successfully shipped commercially shortly before Windows NT 3.1 shipped. I started working on Windows NT 3.1 while it was still in Beta and learned quite a lot about the internal workings of Windows particularly with respect to file systems. I met the current NTFS development lead in 1996 (before he was at Microsoft, in fact) when he took my “Developing File Systems for Windows” class (along with 52 other people). I saw him again last month and, as usual, we ended up talking about file systems.

File systems tend to have very long lifespans as software because they deal in managing persistent data. They have strong reliability requirements: when your web browser runs out of memory and crashes, you restart your web browser and the problem goes away but when your file system runs out of memory and crashes, you restart your computer and you figure out what data has been lost. The problems can be persistent and recurring.

The NTFS design was on the cutting edge of file systems development at the time. They had attempted to construct their file systems micro-kernel style, so the file systems ran in separate processes though ultimately they abandoned that approach because it did not perform sufficiently well on the systems they had available at the time. The vestiges of this remain today (they have Fsp or “file system process” routines for handling queued file system operations). Dave Cutler, Windows NT’s architect, was a long-time micro-kernel proponent (he is co-author on a 1973 SOSP paper that describes a micro-kernel before that was the common terminology.) NTFS used a transactional write ahead journal to provide resilience in the face of system crashes, presumably based upon the Cedar file system work several years before. It incorporated fine-grained ACLs (but so did the file system on which I worked - indeed, when I saw their ACL implementation I was quite comfortable with it because it looked like we had both been tracking the exact same POSIX security drafts.)

It had multiple disparate units of file data within a single file (“streams”), something we had implemented but used a different name and with greater restrictions. It had Extended Attributes (from OS/2). It was a case-insensitive file system (required for POSIX.1 compliance). It used extents to describe files (similar to Veritas’ file system at the time). One novel decision was to duplicate file attribute information into their directories, which made directory enumeration with attributes very fast.

Internally, it used (and still uses) a fairly standard flat index table (since most hierarchical file systems are built on top of a flat table, with the hierarchy built logically on top of the flat name space).

It supported “inline” files (so the data is stored in the MFT record, which is the equivalent of the on-disk inode.) It natively used 16-bit UNICODE names for almost everything (the one exception is extended attribute names, which are 8-bit ASCII strings.)

Over time, they have added features: compression support, encryption support, shared ACLs (so there is a table of ACLs rather than copying them into each separate file, a space saving move.)

The NTFS file system provides an interface for transactional storage to application programs, so that you can do things like “rename this set of files atomically” — and that didn’t required they change the on-disk format to do. The Windows Registry, which is a configuration database, is dependent upon NTFS transactions to implement its own transactional interface (note that it doesn’t have to be, it just was easier given the availability of those transactions). Sure you can construct your own transactional monitor, but when the file system provides it you can also script your file system operations to be transactional.

In the late 1980s and early 1990s defragmentation was generally done via external programs, not by the file system. Doing it in the file system is akin to moving complex functionality from user mode to kernel mode, something that is usually discouraged as we try to limit kernel level complexity (since mistakes at kernel level have more serious consequences than at user level.) Similarly, I wouldn't want my file system doing program loading acceleration directly, either - leave it in an application and use the file system to record the relevant information. Have the program loader use that information to make things load faster.

In Windows Vista, NTFS introduced online file system repair operations (I chuckled when I heard someone at FAST 2018 claiming their file system was the first to do it, more than 10 years after the NTFS team implemented it.) Over time they have further improved this ability to repair minor damage online, and their offline recovery tools, on the rare times you must run them, have become adept at repairing damage.

Today in Windows 10, NTFS supports DAX storage, which is certainly cutting edge (I can’t even easily obtain storage class memory for my own file systems research,) yet it continues to provide rock solid behavior. It is actively maintained, so I know if I report a bug it will get fixed (and I have reported and had bugs fixed in NTFS several times over the years.)

Last I looked, Windows does not have a directory name lookup cache (DNLC) which does slow down open performance. In my own file systems work on Windows in the past, I used to use a fast lookup cache for this - a single entry cache per CPU demonstrated an ~80% hit rate at the time because of the Win32 to NT native API mismatch (the native API is dominated by handle operations, the Win32 API is dominated by name operations.) That’s not a file system issue as much as it is an OS issue and over the years the number of name based APIs at native level have increased and the number of handle based APIs at Win32 level have also increased.

Performance of file systems is often a function of the implementation, not the on-disk structure. File systems designed for disk drives need to be tuned/modified to work with SSDs (for example). Similarly things like the page cache become a drag on performance when you direct access memory.

For a 28 year old design, I would say that NTFS has held up rather well. Can we do better now using modern hardware, advances in our understanding of failures, and the 10,000x increase in the size of our storage and memories? I’d be rather surprised if we could not. But it will take ten years or more before the file system we build today for today’s hardware will be “ready for prime time”. So if you want to build a new file system, don’t plan for today’s hardware. Plan for the hardware that will be available 10 years from now.

Bottom line: NTFS remains an excellent example of 1980s journaling file systems design, balancing features and performance.
